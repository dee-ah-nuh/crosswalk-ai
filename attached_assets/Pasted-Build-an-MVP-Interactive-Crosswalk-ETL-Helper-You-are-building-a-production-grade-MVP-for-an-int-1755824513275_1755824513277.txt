Build an MVP: Interactive Crosswalk & ETL Helper

You are building a production-grade MVP for an internal data engineering tool that helps map (“crosswalk”) client source columns to our standardized data model, validate with regex rules, and export the mappings to be used by our ETL. It must run entirely in Replit with one-click start.

Goal (What this app must do)

Upload a source file (CSV/XLSX) or paste a list of source column names when we don’t have the file yet.

Show an interactive grid: rows = source columns; columns = mapping info (4–5 key fields); allow mapping to our data model fields plus add custom fields not in the model.

Attach regex rules to a source column (zero or many). Provide a built-in regex tester on sample values.

If the raw file is already in our warehouse, expose a button to fetch a data sample by calling our existing stored procedure (configurable; see below).

Show “mapping completeness” (e.g., % mapped) and “validation status” (regex pass/fail counts).

Export:

Crosswalk CSV/XLSX (same shape as our current sheet)

JSON config for pipelines

SQL to create/update crosswalk tables (idempotent)

Persist projects locally (SQLite) so users can return to prior sessions.

Tech Stack (keep it simple)

Frontend: React + Vite, TypeScript, Tailwind. Use a spreadsheet-like grid (e.g., ag-Grid or TanStack Table with editable cells, drag-drop, filter).

Backend: Python FastAPI with SQLite (via SQLAlchemy).

Parsing: pandas for CSV/XLSX. openpyxl for Excel.

Regex: Python re on the backend; JavaScript RegExp in the UI tester.

Warehouse (optional sample fetch): Add a pluggable service to call a stored procedure (Snowflake). Keep it behind a feature toggle/env vars.

Data Model (SQLite)

Create these tables:

data_model_fields

id INTEGER PK
model_table TEXT NOT NULL
model_column TEXT NOT NULL
description TEXT
data_type TEXT
required BOOLEAN DEFAULT 0
unique_key BOOLEAN DEFAULT 0


source_profiles

id INTEGER PK
name TEXT NOT NULL           -- project/profile name
client_id TEXT               -- e.g., 'CCA'
created_at DATETIME
updated_at DATETIME
has_physical_file BOOLEAN DEFAULT 0
raw_table_name TEXT          -- optional reference


source_columns

id INTEGER PK
profile_id INTEGER FK -> source_profiles.id
source_column TEXT NOT NULL
sample_values_json TEXT      -- JSON array of strings
inferred_type TEXT           -- string/number/date/boolean


crosswalk_mappings

id INTEGER PK
profile_id INTEGER FK -> source_profiles.id
source_column_id INTEGER FK -> source_columns.id
model_table TEXT NOT NULL
model_column TEXT NOT NULL
is_custom_field BOOLEAN DEFAULT 0
custom_field_name TEXT       -- when is_custom_field = 1
transform_expression TEXT    -- freeform or DSL (see below)
notes TEXT


regex_rules

id INTEGER PK
source_column_id INTEGER FK -> source_columns.id
rule_name TEXT
pattern TEXT                 -- raw regex string
flags TEXT                   -- e.g., 'i,m'
description TEXT


warehouse_configs (one row per environment)

id INTEGER PK
profile_id INTEGER FK -> source_profiles.id
engine TEXT                 -- 'snowflake'
account TEXT
user TEXT
role TEXT
warehouse TEXT
database TEXT
schema TEXT
private_key_path TEXT       -- optional
password_secret TEXT        -- optional
stored_procedure_call TEXT  -- e.g., "CALL DEV.CONFIG.SP_GET_SAMPLE('<RAW_TABLE>', 50)"
enabled BOOLEAN DEFAULT 0

Frontend UX
Main layout

Left: Project sidebar (create/select profile; set client_id).

Center: Crosswalk Grid with these columns (editable):

Source Column (locked)

Model Table (select from data_model_fields.model_table)

Model Column (dependent select filtered by Model Table)

Custom? (checkbox)

Custom Field Name (text; enabled only if Custom? = true)

Transform (DSL) (text; see DSL below)

Regex Rules (pill list; click to open regex modal)

Mapped? (computed boolean)

Right: Detail Panel (tabs):

Regex Tester (attach rules; test against sample values)

Sample Data (shows up to N values; if warehouse enabled, “Fetch sample from SP”)

Validation Summary (# mapped, # unmapped, regex pass/fail counts)

Exports (buttons to download CSV/XLSX/JSON/SQL)

Workflow (two modes)

Mode A: With file

Upload CSV/XLSX → backend infers columns & basic types → populate source_columns with sample_values_json.

User maps columns and/or adds custom fields.

Optional: click Fetch Sample to call stored proc for RAW table preview.

Mode B: Schema only (no file)

Paste/enter a list of column names (one per line) → create source_columns (no sample values yet).

Mapping UI still works; sample data becomes available later when file exists.

Grid niceties

Quick-filter/search over source & model names.

Keyboard navigation; bulk fill (down-fill) for Model Table.

Badge color for Mapped? and warning icon for required model fields that remain unmapped.

Transform DSL (simple & safe)

Support a minimal expression syntax (stored in transform_expression) that the backend can evaluate deterministically:

Literals: 'text', numbers, NULL

Column reference: col("SOURCE_COLUMN")

Functions: upper(), lower(), trim(), substr(col, start, len), coalesce(a,b), regex_extract(col, 'pattern', group), regex_replace(col, 'pattern', 'repl')

Operators: + (concat for strings), || (concat), CASE WHEN cond THEN a ELSE b END simplified as: if(cond, a, b)

Predicates: matches(col, 'pattern'), is_null(col)

The backend should provide a safe interpreter for this DSL and a translator to SQL snippets for the Export SQL.

API (FastAPI)

POST /profiles → create/select profile

GET /profiles/{id}

POST /profiles/{id}/source/ingest-file (multipart) → parse columns + sample values

POST /profiles/{id}/source/ingest-schema (json: ["colA","colB",...])

GET /profiles/{id}/source-columns

PUT /crosswalk (bulk upsert rows with mapping + transform + custom flag)

POST /source-columns/{id}/regex (create/update rule)

GET /source-columns/{id}/regex/test?value=... → returns pass/fail + groups

POST /profiles/{id}/sample/fetch → calls warehouse stored procedure if enabled; returns json rows (limit N)

GET /profiles/{id}/export/csv → crosswalk CSV/XLSX

GET /profiles/{id}/export/json → JSON config (see schema below)

GET /profiles/{id}/export/sql → SQL script (idempotent DDL/DML)

Export Formats
JSON config (example)
{
  "client_id": "CCA",
  "profile": "2025-08-21-cca-v1",
  "mappings": [
    {
      "source_column": "PATIENT_ID",
      "target": {"table": "CLAIM", "column": "MEMBER_ID"},
      "custom": false,
      "transform": "trim(col('PATIENT_ID'))",
      "regex_rules": [
        {"name": "digitsOnly", "pattern": "^[0-9]+$"}
      ]
    },
    {
      "source_column": "PLAN_CODE",
      "target": {"table": "CLAIM", "column": "PLAN_CODE"},
      "custom": true,
      "custom_field_name": "PLAN_CODE_SRC",
      "transform": "upper(col('PLAN_CODE'))",
      "regex_rules": []
    }
  ]
}

SQL export

Create table if not exists for a CROSSWALK table with columns:

client_id, source_column, model_table, model_column, is_custom_field, custom_field_name, transform_expression, regex_json

Upsert all rows using a deterministic key: (client_id, source_column).

Generate view or CTE snippets that translate DSL → SQL for downstream pipelines.

Warehouse Sample (optional)

If warehouse_configs.enabled = true, allow POST /profiles/{id}/sample/fetch to run a stored procedure string template, e.g.:

Template: CALL DEV_ONSHORE.CONFIG.SP_GET_RAW_SAMPLE('{RAW_TABLE}', 50);

Replace {RAW_TABLE} from source_profiles.raw_table_name.

Put all connection details in env vars; fail gracefully if not configured.

Seeding (for demo)

Seed data_model_fields with ~15 example fields across 2–3 tables, e.g.:

Table CLAIM: (CLAIM_ID, MEMBER_ID, CLAIM_TYPE, SERVICE_DATE, AMOUNT)

Table PROVIDER: (NPI, PROVIDER_NAME, TAXONOMY)

Table PLAN: (PLAN_CODE, PLAN_NAME)
Mark a couple as required=1.

Provide a tiny sample CSV:

PATIENT_ID,CLAIMTYPE,SVC_DT,CHARGE,NPI_NUM,DR_NAME,PLAN_CODE
12345,PHARM,2025-01-03,19.99,5551212,SMITH,AB12


Auto-infer source_columns and sample values on upload.

Validation & States

Compute mapped% = (# rows where target is set or custom field name provided) / total.

For each regex rule, compute pass/fail counts from sample values.

Visually warn when required model fields remain unmapped.

Security & Quality

No external calls unless warehouse sample is explicitly enabled.

Validate and sandbox DSL evaluation.

Unit tests for: DSL translator, regex tester, export builders.

Deliverables

A working Replit project: single “Run” starts both FastAPI and React (use a simple Procfile or npm run dev + uvicorn with a small launcher script).

Clean, minimal UI (Tailwind) with an editable grid, regex modals, and export buttons.

Example seed data + example CSV to demo end-to-end.

README with quickstart and env variables (WAREHOUSE_ENABLED, SNOWFLAKE_*, SP_TEMPLATE).

Stretch (nice-to-have, only if quick)

Drag-connect from Source Column to Target Column (visual mapping lines).

“Suggest Mapping” button using simple fuzzy match (PATIENT_ID → MEMBER_ID).

“Generate SQL Preview” inline for a selected row (show DSL→SQL translation).

Don’ts

Don’t require warehouse connectivity to use the app.

Don’t lock users into having a file—schema-only mode must work.

Don’t over-engineer auth or RBAC for MVP.

Acceptance Criteria (checklist)

 Upload file OR paste schema list and see all source columns in grid.

 Map to model fields; add custom fields; set transforms; attach regex.

 Regex tester works and shows pass/fail on sample values.

 Optional SP sample fetch returns rows when env is set; otherwise disabled with tooltip.

 Export CSV/XLSX, JSON, and SQL all succeed and reflect current mappings.

 Mapped% and required-field warnings are visible and accurate.

 Data persists across reloads (SQLite).